\documentclass{article}

\usepackage{amsmath}

\usepackage{lmodern}

\usepackage{graphicx} 

\usepackage{fancyhdr}

\usepackage{subcaption}

\usepackage[margin=1.2in]{geometry} 

\setlength\headheight{10pt} 

\renewcommand{\figurename}{Figura}


\pagestyle{fancy}
\fancyhf{}
\cfoot{\thepage}
\rhead{Nicolás Valdés \\ RUT: 19.247.388-8 \\ FI3104-1 2018B \\ 27/09/18}
\lhead{\includegraphics[scale=0.52]{logo}}



\begin{document}
\thispagestyle{fancy}
\text{} \vspace{0.3cm}
\begin{center}
\LARGE {\bf Tarea 1} 
\end{center}
\normalsize 
\section*{Problema 1}
\subsection*{Introducción} 
La definición usual de la derivada $f'(x)$ de una función $f(x)$ involucra tomar un límite,  que por su naturaleza es un proceso {\it continuo}. Claramente al calcular derivadas con un computador, esto no es factible. Una opción simple para aproximar una derivada es 
\begin{align}
f'(x)\approx \frac{f(x+h)-f(x)}{h} \equiv f_1'(x), \label{1}
\end{align} 
lo cual sale a partir de hacer una expansión de Taylor para $f$, y despejar $f'(x)$, truncando términos de orden $h$ o mayor. La idea es tomar un $h$ pequeño para que esta aproximación funcione. Si uno se queda con más términos en la serie de Taylor y evalúa series de la función en distintos puntos, puede llegar a una aproximación que trunca términos de orden $h^4$ o mayor:
\begin{align}
f'(x)\approx \frac{-f(x+2h)+8f(x+h)-8f(x-h)+f(x-2h)}{12h}  \equiv f_4'(x).  \label{4}
\end{align}
Este problema se trata de comparar estas dos aproximaciones. 

\subsection*{Metodología} 

Para comparar las aproximaciones, la idea intuitiva es ver ``cuán cerca'' están a la derivada real de una función. Entonces hay que escoger una función y evaluar su derivada de tres maneras, en algún punto $x_0$. Debemos evaluar su derivada real, evaluar $f_1'(x_0)$, y evaluar $f_4'(x_0)$. Usamos $f(x)=-\cos(x)$. Sabemos, desde antes, que $f'(x_0)=\sin(x_0)$. Para ver ``cuán buena'' es una aproximación de $f'(x)$, calculamos $\Delta_i \equiv |f'(x_0)-f_i'(x_0)|$, donde $i\in\{1,4\}$. Esto nos indica de alguna forma el error de un método de aproximación, y es nuestro criterio de comparación. 

En el código, 
\begin{enumerate}
\item Definimos una función que calcula $f_1'$. Tiene tres argumentos: la función a derivar, el punto donde se evalúa la derivada, y el $h$ usado en la ecuación (\ref{1}). Hacemos lo mismo con $f_4'$. 
\item Usamos $x_0=1.388$ (por mi RUT) y $h=(10^{-1},10^{-2},...,10^{-15})$. Evaluamos las funciones del paso 1 en estos valores, y en la función $\cos(x)$.
\item  Calculamos el error $\Delta_i$ restando las $f_i$ del paso anterior, a la función $\sin(x_0)$ incluida en {\it numpy}. 
\item Graficamos $\Delta_i$ como función de $h$, con escala logarítmica en ambos ejes. 
\item Repetimos todo lo anterior, pero cambiando $x_0$ y $h$ para que sean Float 32, o Float 128. 
\end{enumerate}

\subsection*{Resultados} 
En la Figura 1 se encuentran los gráficos producidos para el error $\Delta$, evaluado usando distintos $h$. 
\begin{figure}[ht!]
\begin{subfigure}{0.5\textwidth}
\includegraphics[width=3.3in]{PlotP1_Float32}
\caption{Float32}
\end{subfigure} \begin{subfigure}{0.5\textwidth}
\includegraphics[width=3.3in]{PlotP1_Float64}
\caption{Float64}
\end{subfigure}\\
\begin{subfigure}{\textwidth}
\centering
\includegraphics[width=3.3in]{PlotP1_Float128}
\caption{Float128}
\end{subfigure}
\caption{Error $\Delta$ en función de $h$}
\end{figure}


\subsection*{Conclusiones} 
El comportamiento de $\Delta_i$ no es tan simple. Uno esperaría que el error disminuye a medida que $h$ se hace más chico, dado que disminuye el efecto de despreciar términos de mayor orden en $h$. El problema es que el computador no tiene infinita precisión en los números que calcula. Por ejemplo, la precisión de float64 es del orden de 10$^{-15}$. Para números con muchos decimales, eventualmente el computador trunca el número. Entonces con cada método si restamos algo en el numerador, el resultado no va a ser ``honesto'', va a ser algo con precisión truncada. Pero esto es dividido por un $h$ muy pequeño, entonces este error chico se agranda. Esto se ve dramáticamente en la figura 1(a), donde el error del método $\mathcal{O}(h^4)$ alcanza $10^6$. El error probablemente no aumenta más para el método $\mathcal{O}(h)$ en este caso porque el numerador simplemente arroja el valor 0, resultando en un error constante de orden 1. 

Para $h$ ``grande'' (i.e. $h\geq 10^{-7}$), la precisión del método $\mathcal{O}(h^4)$ suele ser mejor. Esto tiene sentido, dado que lo que hace este método es justamente despreciar menos potencias de $h$. Si $h$ es ``grande'', despreciar términos con $h^2$ ya puede tener efectos notorios, y la precisión del método $\mathcal{O}(h)$ disminuye sustancialmente.  Con las figuras 1(b) y 1(c) se ve que la máxima precisión alcanzada por el método $\mathcal{O}(h)$ es en $h\approx 10^{-8}$. Esta misma precisión se alcanza con $\mathcal{O}(h^4)$ con $h\approx 10^{-2}$. Aquí se ve heurísticamente la forma en que (para $h$'s que el computador puede procesar) el método $\mathcal{O}(h^4)$ tiene más precisión que $\mathcal{O}(h)$. 

\newpage

\section*{Problema 2}
\subsection*{Introducción} 
\subsection*{Metodología} 
\subsection*{Resultados} 
\subsection*{Conclusiones} 






\end{document} 